<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome | MetaLab</title>
    <link>https://langcog.github.io/metalab/documentation/</link>
      <atom:link href="https://langcog.github.io/metalab/documentation/index.xml" rel="self" type="application/rss+xml" />
    <description>Welcome</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 26 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://langcog.github.io/metalab/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Welcome</title>
      <link>https://langcog.github.io/metalab/documentation/</link>
    </image>
    
    <item>
      <title>Getting Started</title>
      <link>https://langcog.github.io/metalab/documentation/getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://langcog.github.io/metalab/documentation/getting-started/</guid>
      <description>&lt;p&gt;MetaLab has many features and ways to access information and even contribute. The menu on the left lets you navigate the existing documentation, if questions remain, please &lt;a href=&#34;https://langcog.github.io/metalab/team/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get in touch&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; This page is under revision, access the previous tutorial &lt;a href=&#34;https://langcog.github.io/metalab2/documentation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contribution Challenge</title>
      <link>https://langcog.github.io/metalab/documentation/challenge/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://langcog.github.io/metalab/documentation/challenge/</guid>
      <description>


&lt;div id=&#34;contribution-challenge-2020-winners&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contribution Challenge 2020 Winners&lt;/h2&gt;
&lt;p&gt;The 2020 contribution challenge has closed and we received a record number of 9 submissions, thanks to all who joined and to the Fetzer Franklin Fund for their support. We are working on making the data available on MetaLab.&lt;/p&gt;
&lt;p&gt;The three winners are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Gabrielle Strouse (University of South Dakota) with a meta-analysis on whether infants learn from video just as well as from live interaction, and the results are already published: doi.org/10.1111/cdev.13429&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Z. L. Zhou (UCLA) with a meta-analysis on the time course of native-language phonotactic learning; stay tuned for a full paper.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Loretta Gasparini (European Master’s in Clinical Linguistics+) on infants’ preference for their native language and ability to discriminate between languages and accents, a preprint is available here: doi.org/10.31219/osf.io/rmn5x&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;the-contribution-challenge-2020-call&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The Contribution Challenge 2020 Call&lt;/h4&gt;
&lt;p&gt;To celebrate the latest (ongoing) upgrade and the inclusion of 25 meta-analyses to date, the MetaLab team with support of the Fetzer Franklin Fund is organizing a challenge for authors of meta-analyses on cognitive development (data challenge) and for contributors of shiny apps (code challenge). The contribution challenge will offer three $1,000 US in cash prizes distributed between (teams of) authors or coders, who contribute meta-analysis data to the MetaLab database or, who make substantial contributions to the site, for example proposing and implementing a shiny app providing a new analysis or functionality in coordination with the MetaLab team. Deadline is 2020-10-15 (15 October).
You can find more information here: &lt;a href=&#34;https://bit.ly/MetaLabChallenge2020&#34; class=&#34;uri&#34;&gt;https://bit.ly/MetaLabChallenge2020&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contribution-challenge-2018-winners&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contribution Challenge 2018 Winners&lt;/h2&gt;
&lt;p&gt;The MetaLab challenge calling for meta-analyses on cognitive development, with support from Berkeley Initiative for Transparency in the Social Sciences (BITSS), has closed. We received data for 7 meta-analyses, which will be added to MetaLab in the coming months.&lt;/p&gt;
&lt;p&gt;The winners are three early career researchers: Angeline Tsui (Ottawa / Stanford), M. Julia Carbajal (LSCP Paris), and Katie Von Holzen (LPP Paris / Maryland).&lt;/p&gt;
&lt;p&gt;Angeline Tsui contributed data on a meta-analysis of the “Switch Task”, a key paradigm in language acquisition research. In a switch task infants are taught new labels for unknown objects (such as “lif” vs “neem”). Their knowledge is then tested by whether they can detect the switching of the word-object pairings (calling the “lif” now “neem”). Results from switch task studies raised the possibility that there are differences in infants’ abilities to distinguish speech sounds in a pure speech perception task (where no visual information giving cues to the referent is presented) versus in a word learning context, and led to a string of follow-up studies that are synthesized in this meta-analysis. Angeline’s paper describing the meta-analysis in more detail is currently under review in Developmental Psychology (&lt;a href=&#34;https://osf.io/uwe8g/&#34;&gt;Preprint&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;M. Julia Carbajal conducted a meta-analysis on infants’ ability to distinguish frequent from rare words (like “hello” versus “hallux”) when these words are just presented via a speech stream without visual referents. In this type of studies, researchers typically compare how long infants like to listen to different word lists (one with very frequent and one with very rare words), which is an easy to apply but very indirect measure. Studies on infants’ ability to distinguish those word lists were the first to establish when infants begin to systematically learn words in their native language, albeit with varying results across studies. It was thus a good moment to estimate the meta-analytic effect size. The paper on this meta-analysis is currently in preparation.&lt;/p&gt;
&lt;p&gt;Katie Von Holzen’s meta-analysis (conducted in collaboration with MetaLab team member Christina Bergmann, which led to 50% of her data being discounted) addresses infants’ sensitivity to mispronunciations (for example, whether “tog” is a good label for “dog”). Dealing with mispronunciations is another key skill in language acquisition and processing, and the meta-analysis aims to show whether infants become more strict or more lenient with experience as to how a word should sound. A short report on the meta-analysis is appearing in the Proceedings of the Cognitive Science Society Conference (&lt;a href=&#34;http://osf.io/nvc8m&#34;&gt;Preprint&lt;/a&gt;), a full-length paper is in preparation.&lt;/p&gt;
&lt;p&gt;We would also like to specifically highlight the contribution of Hugh Rabagliati, Brock Ferguson, and Casey Lew-Williams, who would have been among the winners based on their contribution, but generously stepped down to leave the prize for an early career researcher. The meta-analysis they contributed addresses how infants can learn rules that are implicit in their environment. Their open access &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/desc.12704&#34;&gt;paper&lt;/a&gt; just appeared in the journal Developmental Science (Rabagliati, H., Ferguson, B., &amp;amp; Lew-Williams, C. (2018). “The profile of abstract rule learning in infancy: Meta-analytic and experimental evidence”. Developmental Science, DOI: &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1111/desc.12704&#34;&gt;10.1111/desc.12704&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Thank you to everyone who participated in our challenge. MetaLab continues to be open for submissions, we provide further information on the Tutorials page.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-sharing-policy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data-sharing policy&lt;/h2&gt;
&lt;p&gt;Meta-analyses will be added to the MetaLab online database. Users will be able to download your data and potentially re-use it. However, MetaLab requires anyone who uses a dataset to cite the contributors of the dataset, even if there is no publication or pre-print yet. Citation that users should use are available in the &lt;a href=&#34;https://langcog.github.io/metalab/documentation.html#datasets&#34;&gt;documentation of each dataset&lt;/a&gt;, potentially specifying « in preparation », and/or linking to an online repository (such as OSF). Note that we will update these entries as preprints and published papers become available. Learn more by reading our &lt;a href=&#34;https://langcog.github.io/metalab/publications.html&#34;&gt;full citation policy&lt;/a&gt;.
Unpublished meta-analyses shared on MetaLab do not count as publication.&lt;/p&gt;
&lt;p&gt;MetaLab is dynamic: Meta-analysis can be updated, adding new relevant studies when they are published. Contributors can retain control on this for as long as they want to. Two options exists for the curation and review of data. Contributors can choose to be the curator. This means a contributor agrees to be the person responsible for identifying new relevant papers and signaling them to the MetaLab data manager, who will add them to the database. Curators are expected to check the data entered regularly. Curators are part of the MetaLab team and can choose to join discussions regarding e.g. site revamping. Alternatively, contributors can choose to step down completely, and it will be MetaLab’s job to assign a new curator for such a dataset.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>https://langcog.github.io/metalab/documentation/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://langcog.github.io/metalab/documentation/faq/</guid>
      <description>


&lt;p&gt;This page is under revision, access the previous tutorial &lt;a href=&#34;https://langcog.github.io/metalab2/documentation.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Meta-Analysis?</title>
      <link>https://langcog.github.io/metalab/documentation/why_do_a_ma/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://langcog.github.io/metalab/documentation/why_do_a_ma/</guid>
      <description>
&lt;link href=&#34;https://langcog.github.io/metalab/metalab/rmarkdown-libs/vembedr/css/vembedr.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;div id=&#34;why-conduct-a-meta-analysis-ma&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why conduct a meta-analysis (MA)?&lt;/h1&gt;
&lt;p&gt;For a quick overview on what are meta-analysis (MA), please watch this 3-minute video:&lt;/p&gt;
&lt;div class=&#34;vembedr&#34;&gt;
&lt;div&gt;
&lt;iframe src=&#34;https://www.youtube.com/embed/Omnq13QZ-3c&#34; width=&#34;533&#34; height=&#34;300&#34; frameborder=&#34;0&#34; allowfullscreen=&#34;&#34; data-external=&#34;1&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Meta-analyses (MAs) can be useful several purposes, including: (1) &lt;em&gt;theory building and evaluation&lt;/em&gt; and (2) &lt;em&gt;practical decisions during study design&lt;/em&gt;. This section starts with some basics on why MAs are more useful than single studies for those two purposes.&lt;/p&gt;
&lt;div id=&#34;why-are-single-studies-not-enough&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Why are single studies not enough?&lt;/h4&gt;
&lt;p&gt;When thinking about development, we often look at published experiments testing whether infants have specific abilities, for example whether infants treat native vowels differently from non-native ones, and how those abilities change with age (for more details on this specific topic see [here] (/dataset/inphondb-native.html).&lt;/p&gt;
&lt;p&gt;The results of a single experiment cannot answer those questions once and for all: Each experiment measures behavior of a set of infants in a very specific situation, which might not be generalizable to other situations. Moreover, there might be a measurement error in this one-time snapshot of reality. Finally, the literature likely contains some false positives and false negatives, simply due to the way we conduct statistics. With a significance threshold alpha set to .05, every study we run has a 5% chance of telling us that infants can do something when this is not true - this is a &lt;em&gt;false positive&lt;/em&gt;. This likelihood becomes bigger when researchers engage in seemingly innocent and possibly common practices that increase the chance of a false positive, such as running multiple analyses until one is significant. Some people even propose that most published literature consists of false positives! With a beta set to .2, every study we run has a 20% chance of telling us that infants cannot do something when they actually can - this is a &lt;em&gt;false negative&lt;/em&gt;. In fact, many studies are underpowered, meaning they test too few participants, so that a non-significant result is not due to a true lack of effect, but rather to lack of power to detect it. None of these necessarily are due to bad intentions, wrongdoing, or even poor research practices. Reality is complex and thus any one study can only give us a single, noisy estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-can-mas-help&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How can MAs help?&lt;/h4&gt;
&lt;p&gt;MAs may be the cheapest way to assess generalizability and test whether a certain factor matters. Instead of running 10 experiments, 1 on each vowel contrast, we collect 10 studies in the literature into a single analysis! If effects for e.g. native versus non-native vowels differ significantly in the literature as a whole, then we can be more confident that results will generalize to unobserved vowel contrasts.&lt;/p&gt;
&lt;p&gt;MAs are therefore a tool to collaborate across space and time, instead of having one lab invest a lot of resources. This has the automatic benefit that MAs often also cover more varied participant populations than a single study usually can (for notable exceptions, see for example the &lt;a href=&#34;manybabies.github.io&#34;&gt;ManyBabies Studies&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-can-mas-help-with-false-positives&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How can MAs help with false positives?&lt;/h4&gt;
&lt;p&gt;Collecting many study results from different researchers is a way to try and make up for the possibility that biases influenced the outcome. We can even &lt;em&gt;use MAs to check for biases&lt;/em&gt;, such as asking whether a suspicious number of p-values is just below the significance threshold or whether results are systematically skewed in one direction. Why biases matter is wonderfully illustrated here: &lt;a href=&#34;http://www.alltrials.net/news/the-economist-publication-bias/&#34; class=&#34;uri&#34;&gt;http://www.alltrials.net/news/the-economist-publication-bias/&lt;/a&gt;. Checking for biased results is a whole literature on its own, but as a start tools such as p-curving apps are easily available for every researcher. &lt;a href=&#34;http://www.p-curve.com/&#34; class=&#34;uri&#34;&gt;http://www.p-curve.com/&lt;/a&gt; or &lt;a href=&#34;http://shinyapps.org/apps/p-checker/&#34; class=&#34;uri&#34;&gt;http://shinyapps.org/apps/p-checker/&lt;/a&gt; are two well-documented examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-can-mas-help-with-false-negatives&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How can MAs help with false negatives?&lt;/h4&gt;
&lt;p&gt;MAs help in three ways. By pooling data together, we may be able to &lt;em&gt;bring out a small effect&lt;/em&gt; that was too difficult to detect.&lt;/p&gt;
&lt;p&gt;Additionally, we often do not know about these non-significant findings because it is quite difficult to publish them. &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/26186116&#34;&gt;Community-augmented MAs&lt;/a&gt; like those in MetaLab provide a home for &lt;em&gt;unpublished results&lt;/em&gt;, and allow researchers to benefit from the experience of others.&lt;/p&gt;
&lt;p&gt;Finally, MAs help us in experiment design so we can &lt;em&gt;avoid false negatives due to low power&lt;/em&gt;. When the size of an effect is known and with a fixed significance threshold, calculating power is straightforward. Here is a simulation of how all ingredients fit together: &amp;lt;rpsychologist.com/d3/NHST/&amp;gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-mas-help-in-other-ways&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Can MAs help in other ways?&lt;/h4&gt;
&lt;p&gt;The MAs in MetaLab can also help with &lt;em&gt;study design&lt;/em&gt;, because often many design variables have been coded. Examples include the stimuli used, how long trials were, etc. Instead of doing a tiresome literature review, you can find out what is the most common procedure or which is associated with the biggest effect.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
